{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\paull\\AppData\\Local\\Temp\\ipykernel_9248\\1817990826.py\", line 7, in <module>\n",
      "    from torchvision import transforms, datasets\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\__init__.py\", line 6, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\models\\__init__.py\", line 2, in <module>\n",
      "    from .convnext import *\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py\", line 8, in <module>\n",
      "    from ..ops.misc import Conv2dNormActivation, Permute\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\ops\\__init__.py\", line 23, in <module>\n",
      "    from .poolers import MultiScaleRoIAlign\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\ops\\poolers.py\", line 10, in <module>\n",
      "    from .roi_align import roi_align\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\ops\\roi_align.py\", line 4, in <module>\n",
      "    import torch._dynamo\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "c:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data directly from Kaggle and caches the dataset for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Downloading now...\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/474M [00:00<?, ?B/s]\n",
      "  0%|          | 1.00M/474M [00:00<01:50, 4.51MB/s]\n",
      "  1%|          | 3.00M/474M [00:00<00:47, 10.3MB/s]\n",
      "  2%|▏         | 8.00M/474M [00:00<00:20, 23.9MB/s]\n",
      "  2%|▏         | 11.0M/474M [00:00<00:26, 18.4MB/s]\n",
      "  3%|▎         | 14.0M/474M [00:00<00:22, 21.1MB/s]\n",
      "  4%|▍         | 19.0M/474M [00:00<00:17, 27.0MB/s]\n",
      "  5%|▌         | 24.0M/474M [00:01<00:14, 31.6MB/s]\n",
      "  6%|▌         | 29.0M/474M [00:01<00:12, 36.1MB/s]\n",
      "  7%|▋         | 33.0M/474M [00:01<00:12, 37.2MB/s]\n",
      "  8%|▊         | 39.0M/474M [00:01<00:10, 43.3MB/s]\n",
      "  9%|▉         | 44.0M/474M [00:01<00:13, 33.8MB/s]\n",
      " 11%|█         | 50.0M/474M [00:01<00:11, 40.1MB/s]\n",
      " 12%|█▏        | 55.0M/474M [00:01<00:10, 40.0MB/s]\n",
      " 13%|█▎        | 61.0M/474M [00:01<00:09, 45.2MB/s]\n",
      " 14%|█▍        | 66.0M/474M [00:02<00:09, 43.4MB/s]\n",
      " 15%|█▍        | 71.0M/474M [00:02<00:09, 45.6MB/s]\n",
      " 16%|█▌        | 76.0M/474M [00:02<00:09, 46.2MB/s]\n",
      " 17%|█▋        | 81.0M/474M [00:02<00:08, 46.6MB/s]\n",
      " 18%|█▊        | 86.0M/474M [00:02<00:08, 47.9MB/s]\n",
      " 19%|█▉        | 91.0M/474M [00:02<00:08, 46.3MB/s]\n",
      " 20%|██        | 97.0M/474M [00:02<00:08, 48.8MB/s]\n",
      " 22%|██▏       | 102M/474M [00:03<00:13, 28.1MB/s] \n",
      " 24%|██▎       | 112M/474M [00:03<00:09, 40.9MB/s]\n",
      " 25%|██▍       | 118M/474M [00:03<00:08, 41.6MB/s]\n",
      " 26%|██▌       | 123M/474M [00:03<00:08, 43.2MB/s]\n",
      " 27%|██▋       | 128M/474M [00:03<00:08, 42.0MB/s]\n",
      " 28%|██▊       | 134M/474M [00:03<00:07, 46.6MB/s]\n",
      " 29%|██▉       | 139M/474M [00:03<00:07, 44.3MB/s]\n",
      " 30%|███       | 144M/474M [00:03<00:07, 46.0MB/s]\n",
      " 32%|███▏      | 150M/474M [00:04<00:07, 46.3MB/s]\n",
      " 33%|███▎      | 155M/474M [00:04<00:07, 47.6MB/s]\n",
      " 34%|███▍      | 160M/474M [00:04<00:07, 44.2MB/s]\n",
      " 35%|███▌      | 166M/474M [00:04<00:06, 48.9MB/s]\n",
      " 36%|███▌      | 171M/474M [00:04<00:06, 45.5MB/s]\n",
      " 37%|███▋      | 177M/474M [00:04<00:06, 49.5MB/s]\n",
      " 38%|███▊      | 182M/474M [00:04<00:06, 46.4MB/s]\n",
      " 40%|███▉      | 188M/474M [00:04<00:06, 49.9MB/s]\n",
      " 41%|████      | 193M/474M [00:05<00:05, 50.2MB/s]\n",
      " 42%|████▏     | 198M/474M [00:05<00:06, 46.5MB/s]\n",
      " 43%|████▎     | 203M/474M [00:05<00:06, 47.0MB/s]\n",
      " 44%|████▍     | 208M/474M [00:05<00:06, 44.0MB/s]\n",
      " 45%|████▌     | 214M/474M [00:05<00:05, 48.8MB/s]\n",
      " 46%|████▌     | 219M/474M [00:05<00:05, 45.4MB/s]\n",
      " 47%|████▋     | 225M/474M [00:05<00:05, 49.5MB/s]\n",
      " 49%|████▊     | 230M/474M [00:05<00:05, 46.1MB/s]\n",
      " 50%|████▉     | 236M/474M [00:05<00:05, 49.7MB/s]\n",
      " 51%|█████     | 241M/474M [00:06<00:05, 46.6MB/s]\n",
      " 52%|█████▏    | 246M/474M [00:06<00:04, 47.9MB/s]\n",
      " 53%|█████▎    | 251M/474M [00:06<00:05, 44.4MB/s]\n",
      " 54%|█████▍    | 257M/474M [00:06<00:05, 38.5MB/s]\n",
      " 55%|█████▌    | 263M/474M [00:06<00:05, 43.1MB/s]\n",
      " 57%|█████▋    | 268M/474M [00:06<00:05, 42.7MB/s]\n",
      " 58%|█████▊    | 273M/474M [00:06<00:04, 44.8MB/s]\n",
      " 59%|█████▊    | 278M/474M [00:06<00:04, 45.5MB/s]\n",
      " 60%|█████▉    | 283M/474M [00:07<00:04, 43.6MB/s]\n",
      " 61%|██████    | 289M/474M [00:07<00:04, 47.2MB/s]\n",
      " 62%|██████▏   | 294M/474M [00:07<00:04, 45.3MB/s]\n",
      " 63%|██████▎   | 300M/474M [00:07<00:03, 47.4MB/s]\n",
      " 64%|██████▍   | 305M/474M [00:07<00:03, 46.1MB/s]\n",
      " 66%|██████▌   | 311M/474M [00:07<00:03, 46.9MB/s]\n",
      " 67%|██████▋   | 316M/474M [00:07<00:03, 47.1MB/s]\n",
      " 68%|██████▊   | 321M/474M [00:08<00:03, 40.8MB/s]\n",
      " 69%|██████▉   | 327M/474M [00:08<00:03, 44.9MB/s]\n",
      " 70%|███████   | 333M/474M [00:08<00:03, 44.9MB/s]\n",
      " 71%|███████▏  | 338M/474M [00:08<00:03, 46.5MB/s]\n",
      " 73%|███████▎  | 344M/474M [00:08<00:02, 45.9MB/s]\n",
      " 74%|███████▍  | 350M/474M [00:08<00:02, 49.2MB/s]\n",
      " 75%|███████▍  | 355M/474M [00:08<00:02, 46.6MB/s]\n",
      " 76%|███████▌  | 361M/474M [00:08<00:02, 49.7MB/s]\n",
      " 77%|███████▋  | 366M/474M [00:08<00:02, 47.1MB/s]\n",
      " 78%|███████▊  | 371M/474M [00:09<00:02, 48.2MB/s]\n",
      " 80%|███████▉  | 377M/474M [00:09<00:02, 45.0MB/s]\n",
      " 81%|████████  | 382M/474M [00:09<00:02, 42.7MB/s]\n",
      " 82%|████████▏ | 387M/474M [00:09<00:02, 44.8MB/s]\n",
      " 83%|████████▎ | 392M/474M [00:10<00:04, 20.9MB/s]\n",
      " 84%|████████▎ | 396M/474M [00:10<00:05, 14.8MB/s]\n",
      " 85%|████████▌ | 403M/474M [00:10<00:03, 21.3MB/s]\n",
      " 86%|████████▋ | 409M/474M [00:10<00:02, 25.4MB/s]\n",
      " 87%|████████▋ | 414M/474M [00:10<00:02, 27.5MB/s]\n",
      " 88%|████████▊ | 418M/474M [00:11<00:02, 25.9MB/s]\n",
      " 89%|████████▉ | 423M/474M [00:11<00:02, 24.7MB/s]\n",
      " 90%|█████████ | 428M/474M [00:11<00:01, 29.2MB/s]\n",
      " 91%|█████████▏| 433M/474M [00:11<00:01, 28.3MB/s]\n",
      " 92%|█████████▏| 438M/474M [00:11<00:01, 32.8MB/s]\n",
      " 94%|█████████▎| 444M/474M [00:11<00:00, 36.5MB/s]\n",
      " 95%|█████████▍| 449M/474M [00:12<00:00, 39.2MB/s]\n",
      " 96%|█████████▌| 454M/474M [00:12<00:00, 31.8MB/s]\n",
      " 97%|█████████▋| 458M/474M [00:12<00:00, 28.7MB/s]\n",
      " 97%|█████████▋| 462M/474M [00:13<00:00, 16.7MB/s]\n",
      " 99%|█████████▉| 469M/474M [00:13<00:00, 24.1MB/s]\n",
      "100%|██████████| 474M/474M [00:13<00:00, 37.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification\n",
      "License(s): CC0-1.0\n",
      "Downloading muffin-vs-chihuahua-image-classification.zip to ../dataset\n",
      "\n",
      "Dataset downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../dataset'\n",
    "\n",
    "if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n",
    "    print(\"Dataset not found. Downloading now...\")\n",
    "    \n",
    "    %pip install --quiet kaggle\n",
    "    \n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    \n",
    "    !kaggle datasets download -d samuelcortinhas/muffin-vs-chihuahua-image-classification -p {dataset_path}\n",
    "    \n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(os.path.join(dataset_path, 'muffin-vs-chihuahua-image-classification.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)\n",
    "    \n",
    "    os.remove(os.path.join(dataset_path, 'muffin-vs-chihuahua-image-classification.zip'))\n",
    "    \n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Using cached version.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the test, train and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 160\n",
      "Number of validation samples: 40\n",
      "Number of test samples: 200\n"
     ]
    }
   ],
   "source": [
    "# Parameter to enable development mode\n",
    "DEV = True  # Set to True for development mode (200 images), False for full dataset\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Define directories\n",
    "train_dir = '../dataset/train'\n",
    "test_dir = '../dataset/test'\n",
    "\n",
    "# Load datasets\n",
    "full_train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "if DEV:\n",
    "    # Get all labels from the dataset\n",
    "    labels = np.array(full_train_dataset.targets)\n",
    "\n",
    "    # Use NumPy to get indices for each class (Chihuahua and Muffin)\n",
    "    chihuahua_indices = np.where(labels == 0)[0]  # Class 0 is Chihuahua\n",
    "    muffin_indices = np.where(labels == 1)[0]     # Class 1 is Muffin\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(chihuahua_indices)\n",
    "    np.random.shuffle(muffin_indices)\n",
    "\n",
    "    # Select an equal number of images from each class (100 Chihuahuas, 100 Muffins)\n",
    "    selected_chihuahua_indices = chihuahua_indices[:100]\n",
    "    selected_muffin_indices = muffin_indices[:100]\n",
    "\n",
    "    # Combine the selected indices and shuffle again\n",
    "    selected_indices = np.concatenate([selected_chihuahua_indices, selected_muffin_indices])\n",
    "    np.random.shuffle(selected_indices)\n",
    "\n",
    "    # Split the selected indices into training (160) and validation (40)\n",
    "    train_indices = selected_indices[:160]\n",
    "    valid_indices = selected_indices[160:200]\n",
    "\n",
    "    # Create subsets\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    valid_dataset = Subset(full_train_dataset, valid_indices)\n",
    "    \n",
    "    # Similarly, create a subset of the test dataset (first 200 images for testing)\n",
    "    test_dataset = Subset(test_dataset, range(200))\n",
    "    \n",
    "else:\n",
    "    # Split the training dataset into training and validation sets\n",
    "    n_train_examples = int(len(full_train_dataset) * 0.8)\n",
    "    n_valid_examples = len(full_train_dataset) - n_train_examples\n",
    "    train_dataset, valid_dataset = random_split(full_train_dataset, [n_train_examples, n_valid_examples], generator=torch.manual_seed(0))\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.manual_seed(0))\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)  # Shuffle valid data as well\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Print dataset information\n",
    "print(f'Number of training samples: {len(train_dataset)}')\n",
    "print(f'Number of validation samples: {len(valid_dataset)}')\n",
    "print(f'Number of test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(m, seed=0):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data, mean=1, std=0.02, generator=torch.manual_seed(seed))\n",
    "        m.bias.data.fill_(0.01)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du train_loader: 3786 images\n",
      "Taille du valid_loader: 947 images\n",
      "Taille du test_loader: 1184 images\n"
     ]
    }
   ],
   "source": [
    "# Chemins des répertoires train et test\n",
    "train_dir = '../dataset/train'\n",
    "test_dir = '../dataset/test'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                          # Resize les images à 224x224 pixels\n",
    "    transforms.ToTensor(),                                  # Convertir l'image en tenseur\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalisation\n",
    "])\n",
    "\n",
    "# Charger le dataset d'entraînement complet\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "# Définir les proportions pour l'entraînement et la validation\n",
    "train_size = int(0.8 * len(train_data))  # 80% pour l'entraînement\n",
    "valid_size = len(train_data) - train_size  # 20% pour la validation\n",
    "\n",
    "# Diviser le dataset d'entraînement en entraînement et validation\n",
    "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
    "\n",
    "# Charger le dataset de test\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Définir la taille des batches\n",
    "batch_size = 32\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Taille du train_loader: {len(train_loader.dataset)} images\")\n",
    "print(f\"Taille du valid_loader: {len(valid_loader.dataset)} images\")\n",
    "print(f\"Taille du test_loader: {len(test_loader.dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a few samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_batch_example, labels_batch_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "image_batch_example, labels_batch_example = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for ib in range(batch_size):\n",
    "    plt.subplot(batch_size // 4, 4, ib + 1)\n",
    "    \n",
    "    image = image_batch_example[ib].permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Image label = ' + str(labels_batch_example[ib].item()))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassif(nn.Module):\n",
    "    def __init__(self, input_size_linear, num_channels1=16, num_channels2=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2),nn.ReLU(), nn.MaxPool2d(kernel_size=2))\n",
    "        self.hidden = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=2))\n",
    "        self.output = nn.Sequential(nn.Linear(input_size_linear, num_classes), nn.ReLU())\n",
    "        pass \n",
    "       \n",
    "    def forward(self, x):\n",
    "        out1 = self.input(x)\n",
    "        out2 = self.hidden(out1)\n",
    "        out3 = self.output(out2.reshape(out2.shape[0], -1))\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  214754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNClassif(\n",
       "  (input): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (hidden): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=100352, out_features=2, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "num_classes = 2\n",
    "input_size_linear = num_channels2 * 56 * 56\n",
    "model = CNNClassif(input_size_linear, num_channels1, num_channels2, num_classes)\n",
    "\n",
    "# Print the total number of parameters in the model\n",
    "print('Total number of parameters: ', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# initialization of the network's parameters\n",
    "model.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CNNClassif(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_channels3=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, num_channels1, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels1, num_channels2, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels2, num_channels3, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self._initialize_input_size()\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size_linear, 128)  \n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "    def _initialize_input_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)  \n",
    "            out = self.conv_block1(dummy_input)\n",
    "            out = self.conv_block2(out)\n",
    "            out = self.conv_block3(out)\n",
    "            self.input_size_linear = out.view(-1).size(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv_block1(x)\n",
    "        out2 = self.conv_block2(out1)\n",
    "        out3 = self.conv_block3(out2)\n",
    "        \n",
    "        out_flat = self.flatten(out3)\n",
    "        \n",
    "        out_fc1 = F.relu(self.fc1(out_flat))\n",
    "        out_fc2 = self.fc2(out_fc1)\n",
    "        \n",
    "        output = F.sigmoid(out_fc2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cnn_classifier(model, eval_dataloader):\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels\n",
    "            #images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train_val_cnn_classifier(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, verbose=True, seed=0):\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list for storing the training loss over epochs\n",
    "    train_losses = []\n",
    "\n",
    "    best_model = (copy.deepcopy(model_tr), 0)\n",
    "    val_accuracy = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Initialize the training loss for the current epoch\n",
    "        tr_loss = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            #img_vectorize = images.view(images.shape[0], 1*28*28)\n",
    "            y_pred = model_tr.forward(images)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "           \n",
    "            # For each batch, we update the current epoch loss\n",
    "            tr_loss += loss.item()  \n",
    "\n",
    "        # At the end of each epoch, get the average training loss over all batches\n",
    "        tr_loss = tr_loss / len(train_dataloader)\n",
    "        train_losses.append(tr_loss)\n",
    "\n",
    "        accuracy = eval_cnn_classifier(model_tr,valid_dataloader)\n",
    "        if accuracy > best_model[1]:\n",
    "            best_model = (copy.deepcopy(model_tr), accuracy)\n",
    "            \n",
    "        val_accuracy.append(accuracy)\n",
    "        \n",
    "        # Display the training loss\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f}, Accuracy : {:.4f}'.format(epoch+1, num_epochs, tr_loss, accuracy))\n",
    "\n",
    "    \n",
    "    return best_model, train_losses, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_tr, train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val_cnn_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the training loss and validation accuracy over epochs\u001b[39;00m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mtrain_val_cnn_classifier\u001b[1;34m(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, verbose, seed)\u001b[0m\n\u001b[0;32m     49\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Iterate over batches using the dataloader\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#img_vectorize = images.view(images.shape[0], 1*28*28)\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\Documents\\CODE\\MuffinOrChihuahuaCNN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "num_epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training\n",
    "model_tr, train_losses, val_accuracies = train_val_cnn_classifier(model, train_loader, valid_loader, num_epochs, loss_fn, learning_rate, verbose=True)\n",
    "\n",
    "# Display the training loss and validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(torch.arange(num_epochs)+1, train_losses)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(torch.arange(num_epochs)+1, val_accuracies)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
